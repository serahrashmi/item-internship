{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting execution for sheet : TAS-FRA-004(2)\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'output_data_TAS-FRA-004(2).csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Vuxenutbildningen\\Documents\\GitHub\\item-internship\\code\\serah\\T018_TAS-FRA-004(2).ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Vuxenutbildningen/Documents/GitHub/item-internship/code/serah/T018_TAS-FRA-004%282%29.ipynb#W0sZmlsZQ%3D%3D?line=182'>183</a>\u001b[0m \u001b[39m\"\"\" ---------------------------------------------- \"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Vuxenutbildningen/Documents/GitHub/item-internship/code/serah/T018_TAS-FRA-004%282%29.ipynb#W0sZmlsZQ%3D%3D?line=183'>184</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mStarting execution for sheet : \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m item_transformer\u001b[39m.\u001b[39msheetname)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Vuxenutbildningen/Documents/GitHub/item-internship/code/serah/T018_TAS-FRA-004%282%29.ipynb#W0sZmlsZQ%3D%3D?line=184'>185</a>\u001b[0m item_transformer\u001b[39m.\u001b[39;49mexecute()\n",
      "\u001b[1;32mc:\\Users\\Vuxenutbildningen\\Documents\\GitHub\\item-internship\\code\\serah\\T018_TAS-FRA-004(2).ipynb Cell 1\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vuxenutbildningen/Documents/GitHub/item-internship/code/serah/T018_TAS-FRA-004%282%29.ipynb#W0sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m         merged_df\u001b[39m.\u001b[39mdrop(\u001b[39m'\u001b[39m\u001b[39mmerge_column\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vuxenutbildningen/Documents/GitHub/item-internship/code/serah/T018_TAS-FRA-004%282%29.ipynb#W0sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformation_data(region_yaml_data, merged_df, applicable_rule)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Vuxenutbildningen/Documents/GitHub/item-internship/code/serah/T018_TAS-FRA-004%282%29.ipynb#W0sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m         merged_df\u001b[39m.\u001b[39;49mto_csv(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfile_name, index\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vuxenutbildningen/Documents/GitHub/item-internship/code/serah/T018_TAS-FRA-004%282%29.ipynb#W0sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mExecution completed for sheet : \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msheetname)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vuxenutbildningen/Documents/GitHub/item-internship/code/serah/T018_TAS-FRA-004%282%29.ipynb#W0sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" ---------------------------------------------- \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Vuxenutbildningen\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:3772\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3761\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ABCDataFrame) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_frame()\n\u001b[0;32m   3763\u001b[0m formatter \u001b[39m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3764\u001b[0m     frame\u001b[39m=\u001b[39mdf,\n\u001b[0;32m   3765\u001b[0m     header\u001b[39m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3769\u001b[0m     decimal\u001b[39m=\u001b[39mdecimal,\n\u001b[0;32m   3770\u001b[0m )\n\u001b[1;32m-> 3772\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[39m.\u001b[39;49mto_csv(\n\u001b[0;32m   3773\u001b[0m     path_or_buf,\n\u001b[0;32m   3774\u001b[0m     lineterminator\u001b[39m=\u001b[39;49mlineterminator,\n\u001b[0;32m   3775\u001b[0m     sep\u001b[39m=\u001b[39;49msep,\n\u001b[0;32m   3776\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m   3777\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   3778\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m   3779\u001b[0m     quoting\u001b[39m=\u001b[39;49mquoting,\n\u001b[0;32m   3780\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   3781\u001b[0m     index_label\u001b[39m=\u001b[39;49mindex_label,\n\u001b[0;32m   3782\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[0;32m   3783\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[0;32m   3784\u001b[0m     quotechar\u001b[39m=\u001b[39;49mquotechar,\n\u001b[0;32m   3785\u001b[0m     date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[0;32m   3786\u001b[0m     doublequote\u001b[39m=\u001b[39;49mdoublequote,\n\u001b[0;32m   3787\u001b[0m     escapechar\u001b[39m=\u001b[39;49mescapechar,\n\u001b[0;32m   3788\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m   3789\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Vuxenutbildningen\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1186\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1165\u001b[0m     created_buffer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1167\u001b[0m csv_formatter \u001b[39m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1168\u001b[0m     path_or_buf\u001b[39m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1169\u001b[0m     lineterminator\u001b[39m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1184\u001b[0m     formatter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfmt,\n\u001b[0;32m   1185\u001b[0m )\n\u001b[1;32m-> 1186\u001b[0m csv_formatter\u001b[39m.\u001b[39;49msave()\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1189\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Users\\Vuxenutbildningen\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:240\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[39mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[39m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[0;32m    241\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilepath_or_buffer,\n\u001b[0;32m    242\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    243\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    244\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,\n\u001b[0;32m    245\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompression,\n\u001b[0;32m    246\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstorage_options,\n\u001b[0;32m    247\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[0;32m    248\u001b[0m     \u001b[39m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m csvlib\u001b[39m.\u001b[39mwriter(\n\u001b[0;32m    250\u001b[0m         handles\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m    251\u001b[0m         lineterminator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    256\u001b[0m         quotechar\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquotechar,\n\u001b[0;32m    257\u001b[0m     )\n\u001b[0;32m    259\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\Users\\Vuxenutbildningen\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39mioargs\u001b[39m.\u001b[39mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[39m=\u001b[39merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'output_data_TAS-FRA-004(2).csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "EXCEL_SHEET_NAME = \"ATO Workbook (TRANSPORT ACTIVITY & SERVICES (TAS))2023.xlsx\"\n",
    "REGION_YAML_FILE_NAME = \"regions_roadmap.yaml\"\n",
    "RULEBOOK_YAML_FILE_NAME = \"sources.yaml\"\n",
    "OUTPUT_FILE_NAME = \"output_data_TAS-FRA-004(2).csv\"\n",
    "\n",
    "\n",
    "class ItemTransformer:\n",
    "\n",
    "    def __init__(self, sheetname, file_name):\n",
    "        self.sheetname = sheetname\n",
    "        self.file_name = file_name\n",
    "\n",
    "    def execute(self):\n",
    "        excel_file = pd.ExcelFile(EXCEL_SHEET_NAME)\n",
    "        region_yaml_data = self.load_yaml_to_cache(REGION_YAML_FILE_NAME)\n",
    "        rule_book_yaml_data = self.load_yaml_to_cache(RULEBOOK_YAML_FILE_NAME)\n",
    "        for sheet_name in excel_file.sheet_names:\n",
    "            if sheet_name in [self.sheetname]:\n",
    "                df = pd.read_excel(excel_file, sheet_name=sheet_name, header=None, index_col=None)\n",
    "                df = self.drop_empty_data(df)\n",
    "                df_metadata_columns = self.load_metadata(df)\n",
    "                df_data = self.load_data(df)\n",
    "                df_data.to_csv('internal/internal_df_data.csv', index=False, header=False)\n",
    "                meta_data, applicable_rule = self.transform_metatdata(df_metadata_columns, rule_book_yaml_data)\n",
    "                df_metadata_transformed = pd.DataFrame(meta_data)\n",
    "                df_data_csv = pd.read_csv('internal/internal_df_data.csv')\n",
    "                self.create_dummy_merge_column(df_metadata_transformed, df_data_csv)\n",
    "                merged_df = pd.merge(df_metadata_transformed, df_data_csv, on='merge_column', how='right')\n",
    "                merged_df.drop('merge_column', axis=1, inplace=True)\n",
    "                self.transformation_data(region_yaml_data, merged_df, applicable_rule)\n",
    "                merged_df.to_csv(self.file_name, index=False)\n",
    "        print(\"Execution completed for sheet : \"+ self.sheetname)\n",
    "        \"\"\" ---------------------------------------------- \"\"\"\n",
    "\n",
    "    def load_metadata(self, df):\n",
    "        df_metadata_rows = df.iloc[:8, :]\n",
    "        df_metadata_columns = df_metadata_rows.iloc[:, [0, 1]]\n",
    "        return df_metadata_columns\n",
    "\n",
    "    def load_data(self, df):\n",
    "        df_data_rows = df.iloc[12:64, :]\n",
    "        df_data = df_data_rows.iloc[:, 0:35]\n",
    "        return df_data\n",
    "\n",
    "    def transform_metatdata(self, df_metadata_columns, rule_book_yaml_data):\n",
    "\n",
    "        meta_data = {}\n",
    "        source_value = None\n",
    "        id_value = None\n",
    "        applicable_rule = None\n",
    "        indicator = \"\"\n",
    "        for row in df_metadata_columns._values:\n",
    "            if row[0] == \"Indicator ATO Code:\":\n",
    "                source_value = [row[1]]\n",
    "            elif row[0] == \"Mode:\":\n",
    "                mode_value = [row[1]]\n",
    "            elif row[0] == \"Indicator:\":\n",
    "                key, value = self.find_matching_rule_by_indicator(row[1], rule_book_yaml_data)\n",
    "                id_value = key\n",
    "                indicator = row[1]\n",
    "                applicable_rule = value\n",
    "        meta_data[\"Source\"] = [applicable_rule[\"Source Prefix\"] + \" \" + source_value[0]]\n",
    "        meta_data[\"Variable\"] = applicable_rule[\"Variable\"]\n",
    "        meta_data[\"Unit\"] = applicable_rule[\"Unit\"]\n",
    "        meta_data[\"Service\"] = applicable_rule[\"Service\"]\n",
    "        meta_data[\"Mode\"] = applicable_rule[\"Mode\"]\n",
    "        meta_data[\"Vehicle Type\"] = self.get_vehicle_type(meta_data[\"Mode\"], indicator, applicable_rule)\n",
    "        meta_data[\"Technology\"] = [\"All\"]\n",
    "        meta_data[\"Fuel\"] = [\"All\"]\n",
    "        meta_data[\"ID\"] = id_value\n",
    "        return meta_data, applicable_rule\n",
    "\n",
    "    def transformation_data(self, yaml_data, merged_df, applicable_rule):\n",
    "        self.rename_and_reorder_columns(merged_df)\n",
    "        unit = applicable_rule[\"Unit Factor\"]\n",
    "        for column_name in merged_df.columns:\n",
    "            no_spaces = column_name.replace(\" \", \"\")\n",
    "            try:\n",
    "                if not no_spaces.isalpha():\n",
    "                    integer_number = int(column_name.split('.')[0])\n",
    "                    if 1900 <= integer_number <= 2022:\n",
    "                        merged_df[column_name] = merged_df[column_name] / unit\n",
    "                        merged_df.rename(columns={column_name: integer_number}, inplace=True)\n",
    "            except Exception as e:\n",
    "                print(f\"There is a exception transforming column : {column_name}\")\n",
    "            self.add_region_from_iso_code(column_name, merged_df, yaml_data)\n",
    "        self.fill_years(merged_df, applicable_rule)\n",
    "\n",
    "\n",
    "    def rename_and_reorder_columns(self, merged_df):\n",
    "        merged_df.rename(columns={'Economy Code': 'ISO Code'}, inplace=True)\n",
    "        merged_df.rename(columns={'Economy Name': 'Country'}, inplace=True)\n",
    "        # Specify the column to be shifted\n",
    "        country_to_shift = 'Country'\n",
    "        ISO_code_to_shift = 'ISO Code'\n",
    "        country_position = 1  # Specify the desired position (index) where the column should be moved\n",
    "        ISO_position = 2  # Specify the desired position (index) where the column should be moved\n",
    "        # Shift the column to the desired position\n",
    "        merged_df.insert(country_position, country_to_shift, merged_df.pop(country_to_shift))\n",
    "        merged_df.insert(ISO_position, ISO_code_to_shift, merged_df.pop(ISO_code_to_shift))\n",
    "        if \"Remarks\" in merged_df.columns:\n",
    "            merged_df.pop(\"Remarks\")\n",
    "\n",
    "    def add_region_from_iso_code(self, column_name, merged_df, yaml_data):\n",
    "        if column_name == \"ISO Code\":\n",
    "            region_list = []\n",
    "            economy_code = merged_df[column_name]\n",
    "            for code in economy_code:\n",
    "                if code == \"nan\":\n",
    "                    region_list.append(\"Not Found\")\n",
    "                match_found = False\n",
    "                for key, value in yaml_data.items():\n",
    "                    countries_economy_code = value[\"countries\"]\n",
    "                    if code in countries_economy_code:\n",
    "                        match_found = True\n",
    "                        region_list.append(key)\n",
    "                        break\n",
    "                if not match_found:\n",
    "                    region_list.append(\"Not Found\")\n",
    "            merged_df.insert(3, \"Region\", region_list)\n",
    "\n",
    "    def fill_years(self, merged_df, applicable_rule):\n",
    "        fill_until_year = 0000\n",
    "        fill_year_from_index = 0\n",
    "        for index, column_name in enumerate(merged_df.columns):\n",
    "            try:\n",
    "                if isinstance(column_name, int):\n",
    "                    fill_until_year = column_name\n",
    "                    fill_year_from_index = index\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"There is a exception transforming column : {column_name}\")\n",
    "        fill_years_from = applicable_rule[\"Fill years from\"]\n",
    "\n",
    "        i = 0\n",
    "        number_of_years_to_fill = fill_until_year - fill_years_from\n",
    "        while i < number_of_years_to_fill:\n",
    "            merged_df.insert(fill_year_from_index, fill_years_from, None)\n",
    "            fill_years_from += 1\n",
    "            i += 1\n",
    "            fill_year_from_index += 1\n",
    "\n",
    "\n",
    "    def create_dummy_merge_column(self, df_metadata_tranformed, df_data_csv):\n",
    "        df_metadata_tranformed['merge_column'] = 1\n",
    "        df_data_csv[\"merge_column\"] = 1\n",
    "\n",
    "    def drop_empty_data(self, df):\n",
    "        # Drop empty columns and unnecessary rows\n",
    "        return df.dropna(axis=1, how='all').dropna(axis=0, how='all')\n",
    "\n",
    "    def load_yaml_to_cache(self, filename):\n",
    "        yaml_file_path = f\"config/{filename}\"\n",
    "        with open(yaml_file_path, 'r') as file:\n",
    "            yaml_data_cache = yaml.safe_load(file)\n",
    "        return yaml_data_cache\n",
    "\n",
    "    def find_matching_rule_by_indicator(self, indicator, rule_book_yaml_data):\n",
    "        match_found = False\n",
    "        for key, inner_dict in rule_book_yaml_data.items():\n",
    "            if inner_dict[\"Name\"] in indicator:\n",
    "                match_found = True\n",
    "                return key, inner_dict\n",
    "        if not match_found:\n",
    "            print(\"Matching rule not found for the indicator present in your sheet\")\n",
    "\n",
    "    def get_vehicle_type(self, mode, indicator, applicable_rule):\n",
    "        generated_vehicle_type = mode[0] + \" \" + indicator.split(' -')[0].lower()\n",
    "        try:\n",
    "            rule_vehicle_type_value = applicable_rule[\"Vehicle Type\"][generated_vehicle_type]\n",
    "            return rule_vehicle_type_value\n",
    "        except KeyError as e:\n",
    "            rule_vehicle_type_value = applicable_rule[\"Vehicle Type\"][\"Default\"]\n",
    "            return rule_vehicle_type_value\n",
    "\n",
    "# Execute transformation for sheet 'TAS-FRA-004(2)'\n",
    "SHEET_NAME = 'TAS-FRA-004(2)'\n",
    "OUTPUT_FILE_NAME = \"output_data_TAS-FRA-004(2).csv\"\n",
    "item_transformer = ItemTransformer(SHEET_NAME, OUTPUT_FILE_NAME)\n",
    "\"\"\" ---------------------------------------------- \"\"\"\n",
    "print(\"Starting execution for sheet : \"+ item_transformer.sheetname)\n",
    "item_transformer.execute()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
