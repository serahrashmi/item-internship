{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting execution for sheet : TAS-VEP-018\n",
      "Unit\n",
      "1000000\n",
      "Execution completed for sheet : TAS-VEP-018\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "EXCEL_SHEET_NAME = \"ATO Workbook (TRANSPORT ACTIVITY & SERVICES (TAS))2023.xlsx\"\n",
    "REGION_YAML_FILE_NAME = \"regions_roadmap.yaml\"\n",
    "RULEBOOK_YAML_FILE_NAME = \"sources.yaml\"\n",
    "OUTPUT_FILE_NAME = \"output_data_TAS-VEP-018.csv\"\n",
    "\n",
    "\n",
    "class ItemTransformer:\n",
    "\n",
    "    def __init__(self, sheetname, file_name):\n",
    "        self.sheetname = sheetname\n",
    "        self.file_name = file_name\n",
    "\n",
    "    def execute(self):\n",
    "        # Read the files from the main folder\n",
    "        excel_file = pd.ExcelFile(EXCEL_SHEET_NAME)\n",
    "        region_yaml_data = self.load_yaml_to_cache(REGION_YAML_FILE_NAME)\n",
    "        rule_book_yaml_data = self.load_yaml_to_cache(RULEBOOK_YAML_FILE_NAME)\n",
    "        for sheet_name in excel_file.sheet_names:\n",
    "            if sheet_name in [self.sheetname]:\n",
    "                df = pd.read_excel(excel_file, sheet_name=sheet_name, header=None, index_col=None)\n",
    "                df = self.drop_empty_data(df)\n",
    "                df_metadata_columns = self.load_metadata(df)\n",
    "                df_data = self.load_data(df)\n",
    "                df_data.to_csv('internal/internal_df_data.csv', index=False, header=False)\n",
    "                meta_data, applicable_rule = self.transform_metatdata(df_metadata_columns, rule_book_yaml_data)\n",
    "                df_metadata_transformed = pd.DataFrame(meta_data)\n",
    "                df_data_csv = pd.read_csv('internal/internal_df_data.csv')\n",
    "                self.create_dummy_merge_column(df_metadata_transformed, df_data_csv)\n",
    "                merged_df = pd.merge(df_metadata_transformed, df_data_csv, on='merge_column', how='right')\n",
    "                merged_df.drop('merge_column', axis=1, inplace=True)\n",
    "                self.transformation_data(region_yaml_data, merged_df, applicable_rule)\n",
    "                merged_df.to_csv(self.file_name, index=False)\n",
    "        print(\"Execution completed for sheet : \"+ self.sheetname)\n",
    "        \"\"\" ---------------------------------------------- \"\"\"\n",
    "\n",
    "    def load_metadata(self, df):\n",
    "        df_metadata_rows = df.iloc[:8, :]\n",
    "        df_metadata_columns = df_metadata_rows.iloc[:, [0, 1]]\n",
    "        return df_metadata_columns\n",
    "\n",
    "    def load_data(self, df):\n",
    "        df_data_rows = df.iloc[12:64, :]\n",
    "        df_data = df_data_rows.iloc[:, 0:35]\n",
    "        return df_data\n",
    "\n",
    "    def transform_metatdata(self, df_metadata_columns, rule_book_yaml_data):\n",
    "\n",
    "        meta_data = {}\n",
    "        source_value = None\n",
    "        id_value = None\n",
    "        applicable_rule = None\n",
    "        indicator = \"\"\n",
    "        for row in df_metadata_columns._values:\n",
    "            if row[0] == \"Indicator ATO Code:\":\n",
    "                source_value = [row[1]]\n",
    "            elif row[0] == \"Mode:\":\n",
    "                mode_value = [row[1]]\n",
    "            elif row[0] == \"Indicator:\":\n",
    "                key, value = self.find_matching_rule_by_indicator(row[1], rule_book_yaml_data)\n",
    "                id_value = key\n",
    "                indicator = row[1]\n",
    "                applicable_rule = value\n",
    "        meta_data[\"Source\"] = [applicable_rule[\"Source Prefix\"] + \" \" + source_value[0]]\n",
    "        meta_data[\"Variable\"] = applicable_rule[\"Variable\"]\n",
    "        meta_data[\"Unit\"] = applicable_rule[\"Unit\"]\n",
    "        meta_data[\"Service\"] = applicable_rule[\"Service\"]\n",
    "        meta_data[\"Mode\"] = applicable_rule[\"Mode\"]\n",
    "        meta_data[\"Vehicle Type\"] = self.get_vehicle_type(meta_data[\"Mode\"], indicator, applicable_rule)\n",
    "        meta_data[\"Technology\"] = [\"All\"]\n",
    "        meta_data[\"Fuel\"] = [\"All\"]\n",
    "        meta_data[\"ID\"] = id_value\n",
    "        return meta_data, applicable_rule\n",
    "\n",
    "    def transformation_data(self, yaml_data, merged_df, applicable_rule):\n",
    "        self.rename_and_reorder_columns(merged_df)\n",
    "        unit = applicable_rule[\"Unit Factor\"]\n",
    "        for column_name in merged_df.columns:\n",
    "            no_spaces = column_name.replace(\" \", \"\")\n",
    "            try:\n",
    "                if not no_spaces.isalpha():\n",
    "                    integer_number = int(column_name.split('.')[0])\n",
    "                    if 1900 <= integer_number <= 2022:\n",
    "                        merged_df[column_name] = merged_df[column_name] / unit\n",
    "                        merged_df.rename(columns={column_name: integer_number}, inplace=True)\n",
    "            except Exception as e:\n",
    "                print(f\"There is a exception transforming column : {column_name}\")\n",
    "            self.add_region_from_iso_code(column_name, merged_df, yaml_data)\n",
    "            self.fill_years(merged_df, applicable_rule)\n",
    "\n",
    "\n",
    "    def rename_and_reorder_columns(self, merged_df):\n",
    "        merged_df.rename(columns={'Economy Code': 'ISO Code'}, inplace=True)\n",
    "        merged_df.rename(columns={'Economy Name': 'Country'}, inplace=True)\n",
    "        # Specify the column to be shifted\n",
    "        country_to_shift = 'Country'\n",
    "        ISO_code_to_shift = 'ISO Code'\n",
    "        country_position = 1  # Specify the desired position (index) where the column should be moved\n",
    "        ISO_position = 2  # Specify the desired position (index) where the column should be moved\n",
    "        # Shift the column to the desired position\n",
    "        merged_df.insert(country_position, country_to_shift, merged_df.pop(country_to_shift))\n",
    "        merged_df.insert(ISO_position, ISO_code_to_shift, merged_df.pop(ISO_code_to_shift))\n",
    "        if \"Remarks\" in merged_df.columns:\n",
    "            merged_df.pop(\"Remarks\")\n",
    "\n",
    "    def add_region_from_iso_code(self, column_name, merged_df, yaml_data):\n",
    "        if column_name == \"ISO Code\":\n",
    "            region_list = []\n",
    "            economy_code = merged_df[column_name]\n",
    "            for code in economy_code:\n",
    "                if code == \"nan\":\n",
    "                    region_list.append(\"Not Found\")\n",
    "                match_found = False\n",
    "                for key, value in yaml_data.items():\n",
    "                    countries_economy_code = value[\"countries\"]\n",
    "                    if code in countries_economy_code:\n",
    "                        match_found = True\n",
    "                        region_list.append(key)\n",
    "                        break\n",
    "                if not match_found:\n",
    "                    region_list.append(\"Not Found\")\n",
    "            merged_df.insert(3, \"Region\", region_list)\n",
    "\n",
    "    def fill_years(self, merged_df, applicable_rule):\n",
    "        fill_until_year = 0000\n",
    "        fill_year_from_index = 0\n",
    "        for index, column_name in enumerate(merged_df.columns):\n",
    "            try:\n",
    "                if isinstance(column_name, int):\n",
    "                    fill_until_year = column_name\n",
    "                    fill_year_from_index = index\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"There is a exception transforming column : {column_name}\")\n",
    "        fill_years_from = applicable_rule[\"Fill years from\"]\n",
    "\n",
    "        i = 0\n",
    "        number_of_years_to_fill = fill_until_year - fill_years_from\n",
    "        while i < number_of_years_to_fill:\n",
    "            merged_df.insert(fill_year_from_index, fill_years_from, None)\n",
    "            fill_years_from += 1\n",
    "            i += 1\n",
    "            fill_year_from_index += 1\n",
    "\n",
    "\n",
    "    def create_dummy_merge_column(self, df_metadata_tranformed, df_data_csv):\n",
    "        df_metadata_tranformed['merge_column'] = 1\n",
    "        df_data_csv[\"merge_column\"] = 1\n",
    "\n",
    "    def drop_empty_data(self, df):\n",
    "        # Drop empty columns and unnecessary rows\n",
    "        return df.dropna(axis=1, how='all').dropna(axis=0, how='all')\n",
    "\n",
    "    def load_yaml_to_cache(self, filename):\n",
    "        yaml_file_path = f\"config/{filename}\"\n",
    "        with open(yaml_file_path, 'r') as file:\n",
    "            yaml_data_cache = yaml.safe_load(file)\n",
    "        return yaml_data_cache\n",
    "\n",
    "    def find_matching_rule_by_indicator(self, indicator, rule_book_yaml_data):\n",
    "        match_found = False\n",
    "        for key, inner_dict in rule_book_yaml_data.items():\n",
    "            if inner_dict[\"Name\"] in indicator:\n",
    "                match_found = True\n",
    "                return key, inner_dict\n",
    "        if not match_found:\n",
    "            print(\"Matching rule not found for the indicator present in your sheet\")\n",
    "\n",
    "    def get_vehicle_type(self, mode, indicator, applicable_rule):\n",
    "        generated_vehicle_type = mode[0] + \" \" + indicator.split(' -')[0].lower()\n",
    "        try:\n",
    "            rule_vehicle_type_value = applicable_rule[\"Vehicle Type\"][generated_vehicle_type]\n",
    "            return rule_vehicle_type_value\n",
    "        except KeyError as e:\n",
    "            rule_vehicle_type_value = applicable_rule[\"Vehicle Type\"][\"Default\"]\n",
    "            return rule_vehicle_type_value\n",
    "\n",
    "\n",
    "# Execute transformation for sheet 'TAS-VEP-018'\n",
    "SHEET_NAME = 'TAS-VEP-018'\n",
    "OUTPUT_FILE_NAME = \"output_data_TAS-VEP-018.csv\"\n",
    "item_transformer = ItemTransformer(SHEET_NAME, OUTPUT_FILE_NAME)\n",
    "\"\"\" ---------------------------------------------- \"\"\"\n",
    "print(\"Starting execution for sheet : \"+ item_transformer.sheetname)\n",
    "item_transformer.execute()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
